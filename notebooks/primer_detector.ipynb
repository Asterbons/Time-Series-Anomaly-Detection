{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c359c8f-797c-43b2-8df0-62d8e38dbc90",
   "metadata": {},
   "source": [
    "<hr/>\n",
    "<div class=\"alert alert-success alertsuccess\" style=\"margin-top: 20px\">\n",
    "[Tip]: To execute the Python code in the code cell below, click on the cell to select it and press <kbd>Shift</kbd> + <kbd>Enter</kbd>.\n",
    "</div>\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1febbc13-4eb9-4804-8b87-56baa1a0cc1f",
   "metadata": {},
   "source": [
    "### Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chubby-variation",
   "metadata": {},
   "outputs": [],
   "source": [
    "from aeon.segmentation import find_dominant_window_sizes\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import fnmatch\n",
    "import zipfile\n",
    "\n",
    "%config InlineBackend.figure_formats = {'png', 'retina'}\n",
    "\n",
    "### Utility functions\n",
    "\n",
    "def read_series(file, locations):\n",
    "    # file is e.g. \"000_Anomaly_2500.csv\"\n",
    "    internal_name = folder_in_zip + file\n",
    "    print(internal_name)\n",
    "\n",
    "    with zf.open(internal_name) as f:\n",
    "        data = pd.read_csv(f, header=None)\n",
    "\n",
    "    data = np.array(data).flatten()\n",
    "\n",
    "    # Extract file name\n",
    "    file_name = file.split('.')[0]\n",
    "    splits = file_name.split('_')\n",
    "    test_start = np.array(splits[-1])\n",
    "\n",
    "    # Extract anomaly location\n",
    "    anomaly = (-1, -1)\n",
    "    if file_name in locations.index:\n",
    "        row = locations.loc[file_name]\n",
    "        anomaly = row[\"Start\"], row[\"End\"]    \n",
    "\n",
    "    return (file_name, int(test_start), data, anomaly)\n",
    "\n",
    "def sliding_window(a, window):\n",
    "    shape = a.shape[:-1] + (a.shape[-1] - window + 1, window)\n",
    "    strides = a.strides + (a.strides[-1],)\n",
    "    return np.lib.stride_tricks.as_strided(a, shape=shape, strides=strides)\n",
    "\n",
    "\n",
    "def visualize_with_anomaly_score(\n",
    "        data, score, test_start, \n",
    "        predicted, anomaly, name = None\n",
    "        ):\n",
    "    '''Input:\n",
    "       data: array with the raw data\n",
    "       test_start: the offset where the test data starts\n",
    "       predicted: The offset of your prediction.\n",
    "       anomaly: The offset of the anomaly. \n",
    "                      If -1 is passed, no anomaly is plottet.\n",
    "    '''\n",
    "    \n",
    "    anomaly_start = anomaly[0]\n",
    "    anomaly_end = anomaly[1]\n",
    "    predicted_start = max(0, predicted - 50)\n",
    "    predicted_end = min(predicted + 50, data.shape[-1])\n",
    "    \n",
    "    fig, ax = plt.subplots(2,1, figsize=(20, 4), sharex=True)\n",
    "    sns.lineplot(x=np.arange(test_start, len(data)), y=data[test_start:], ax = ax[0], lw=0.5, label=\"Test\")\n",
    "    sns.lineplot(x=np.arange(0, test_start), y=data[:test_start], ax = ax[0], lw=0.5, label=\"Train\")\n",
    "        \n",
    "    if (anomaly_start > 0):\n",
    "        sns.lineplot(x=np.arange(anomaly_start, anomaly_end), \n",
    "                     y=data[anomaly_start:anomaly_end], ax = ax[0], label=\"Actual\", color=\"green\")\n",
    "    \n",
    "    sns.lineplot(x=np.arange(len(score)), y=score, ax = ax[1], label=\"Anomaly Scores\")\n",
    "    sns.lineplot(x=np.arange(predicted_start, predicted_end), \n",
    "                 y=data[predicted_start:predicted_end], ax = ax[0], label=\"Predicted\", color=\"red\")\n",
    "\n",
    "    if name is not None:\n",
    "        ax[0].set_title(name)\n",
    "        \n",
    "    sns.despine()\n",
    "\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "locations = pd.read_csv(\"labels.csv\")\n",
    "locations.set_index(\"Name\", inplace=True)\n",
    "\n",
    "zip_path = \"phase_1.zip\"\n",
    "folder_in_zip = \"phase_1/\"\n",
    "\n",
    "# Pre-open the zip for listing files\n",
    "zf = zipfile.ZipFile(zip_path)\n",
    "\n",
    "# All CSV files inside phase_1/ (excluding labels.csv for the widget)\n",
    "file_list = np.sort([\n",
    "    name[len(folder_in_zip):]  # strip \"phase_1/\" so widget shows just the filename\n",
    "    for name in zf.namelist()\n",
    "    if name.startswith(folder_in_zip)\n",
    "       and fnmatch.fnmatch(name, \"*.csv\")\n",
    "       and not name.endswith(\"labels.csv\")\n",
    "])    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "otherwise-calculator",
   "metadata": {},
   "source": [
    "# Exercise 3 - Anomaly Detection\n",
    "\n",
    "## Dataset\n",
    "In this notebook, you must implement methods for detecting anomalies in time series.\n",
    "\n",
    "<img src=\"https://box.hu-berlin.de/f/53a91798173c4dad9345/?dl=1\" width=800/>\n",
    "\n",
    "You are given 30 time series with clean train segments and anomalous test segments:\n",
    "\n",
    "- TRAIN: Normal data only\n",
    "- TEST:  Exactly 1 anomaly per series (unknown size/shape)\n",
    "\n",
    "### TASK: Implement $\\geq 3$ anomaly detection approaches, with two coming from different categories.\n",
    "\n",
    "1. Distance-Based (e.g., k-Nearest Neighbor Classifier)\n",
    "2. Model-Based (e.g., Isolation Forest, LOF, One-Class-SVM)\n",
    "3. Regression-Based (e.g., linear / polynomial regression)\n",
    "4. Forecasting-Based (e.g., Prophet, ARIMA)\n",
    "5. Clustering-Based (e.g., DBSCAN)\n",
    "6. Statistics-Based (e.g., IQR, KDE)\n",
    "\n",
    "**Hints:**\n",
    "- You may use Vibe Coding to solve this exercise.\n",
    "- You may play around with **pre-processing**. **This counts as a different approach**\n",
    "\n",
    "### You must hand in this exercise via Moodle and submit predictions to Kaggle."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adaptive-community",
   "metadata": {},
   "source": [
    "<hr/>\n",
    "\n",
    "# Anomaly Detection using Predictive Modelling\n",
    "\n",
    "You are given wrappers, which you can use to implement methods. Each wrapper returns a score-profile and the predicted anomaly. The score's maximum indicates where the anomaly is.\n",
    "\n",
    "<img src=\"https://box.hu-berlin.de/f/2812ccfbcae24e318f9e/?dl=1\" width=800/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0cac93-64a0-47e8-806a-2daac0631847",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distance-based anomaly scores\n",
    "# e.g. k-Nearest Neighbor Classifier\n",
    "# see: https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html\n",
    "def nearest_neighbor_based_score(X, test_start):\n",
    "    score = np.zeros(len(X))\n",
    "    # TODO: fill in code here as needed \n",
    "\n",
    "    from sklearn.neighbors import NearestNeighbors\n",
    "    # Fit kNN on training data only\n",
    "    knn = NearestNeighbors(n_neighbors=5)\n",
    "    knn.fit(X[:test_start - window_size + 1])\n",
    "    \n",
    "    # For each point, get distance to nearest neighbors in training set\n",
    "    distances, _ = knn.kneighbors(X)\n",
    "    \n",
    "    # Average distance to k neighbors = anomaly score\n",
    "    score = np.mean(distances, axis=1)\n",
    "    \n",
    "    # Pay attention to using argmin or argmax!\n",
    "    predicted = test_start + np.argmax(score[test_start:])\n",
    "    return score, predicted\n",
    "\n",
    "\n",
    "# Use a classification approach to detect anaomalies\n",
    "# e.g. e.g., Isolation Forest, LOF, One-Class-SVM\n",
    "# see: https://scikit-learn.org/stable/modules/outlier_detection.html\n",
    "def classifcation_based_scores(X, test_start):   \n",
    "    score = np.zeros(len(X))\n",
    "    # TODO: fill in code here as needed \n",
    "\n",
    "    from sklearn.ensemble import IsolationForest\n",
    "    from sklearn.svm import OneClassSVM\n",
    "    from sklearn.neighbors import LocalOutlierFactor\n",
    "    from sklearn.preprocessing import RobustScaler\n",
    "    from scipy.ndimage import gaussian_filter1d\n",
    "    import warnings\n",
    "    warnings.filterwarnings('ignore')\n",
    "    \n",
    "    # === 1. Feature Engineering ===\n",
    "    features = []\n",
    "    for i in range(len(X)):\n",
    "        window = X[i]\n",
    "        feat = [\n",
    "            np.mean(window),\n",
    "            np.std(window),\n",
    "            window[-1],\n",
    "            window[-1] - window[0],  # Trend\n",
    "            np.max(window) - np.min(window),  # Range\n",
    "            np.percentile(window, 75) - np.percentile(window, 25),  # IQR\n",
    "        ]\n",
    "        # First derivative features\n",
    "        diffs = np.diff(window)\n",
    "        feat.extend([np.mean(diffs), np.std(diffs), np.max(np.abs(diffs))])\n",
    "        # Second derivative\n",
    "        diffs2 = np.diff(diffs)\n",
    "        if len(diffs2) > 0:\n",
    "            feat.extend([np.mean(diffs2), np.std(diffs2)])\n",
    "        else:\n",
    "            feat.extend([0, 0])\n",
    "        features.append(feat)\n",
    "    \n",
    "    X_features = np.array(features)\n",
    "    \n",
    "    # === 2. Scaling ===\n",
    "    scaler = RobustScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X[:test_start - window_size + 1])\n",
    "    X_all_scaled = scaler.transform(X_features)\n",
    "    \n",
    "    # === 3. Ensemble of Classification-Based Methods ===\n",
    "    all_scores = []\n",
    "    \n",
    "    # Method A: Isolation Forest with different contamination values\n",
    "    for cont in [0.01, 0.05, 0.1]:\n",
    "        try:\n",
    "            iso_forest = IsolationForest(contamination=cont, random_state=42, n_estimators=100)\n",
    "            iso_forest.fit(X_train_scaled)\n",
    "            # decision_function returns negative for outliers, we negate to get higher=anomalous\n",
    "            iso_scores = -iso_forest.decision_function(X_all_scaled)\n",
    "            all_scores.append(iso_scores)\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    # Method B: One-Class SVM\n",
    "    for nu in [0.01, 0.05, 0.1]:\n",
    "        try:\n",
    "            ocsvm = OneClassSVM(nu=nu, kernel='rbf', gamma='auto')\n",
    "            ocsvm.fit(X_train_scaled)\n",
    "            # decision_function returns negative for outliers\n",
    "            svm_scores = -ocsvm.decision_function(X_all_scaled)\n",
    "            all_scores.append(svm_scores)\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    # Method C: LOF (Local Outlier Factor) with novelty detection\n",
    "    for n_neighbors in [10, 20, 30, 50]:\n",
    "        try:\n",
    "            lof = LocalOutlierFactor(n_neighbors=n_neighbors, novelty=True, contamination=0.01)\n",
    "            lof.fit(X_train_scaled)\n",
    "            lof_scores = -lof.decision_function(X_all_scaled)\n",
    "            all_scores.append(lof_scores)\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    # === 4. Normalize and combine scores ===\n",
    "    normalized_scores = []\n",
    "    for s in all_scores:\n",
    "        s_norm = (s - np.min(s)) / (np.max(s) - np.min(s) + 1e-10)\n",
    "        normalized_scores.append(s_norm)\n",
    "    \n",
    "    combined = np.mean(normalized_scores, axis=0)\n",
    "    \n",
    "    # === 5. Heavy smoothing to find anomaly region ===\n",
    "    window_size = max(20, X.shape[1])\n",
    "    score = gaussian_filter1d(combined, sigma=window_size/3)\n",
    "    \n",
    "    # Zero out training region\n",
    "    score[:test_start] = 0\n",
    "    \n",
    "    # Pay attention to using argmin or argmax!\n",
    "    predicted = test_start + np.argmin(score[test_start:])\n",
    "    return score, predicted\n",
    "\n",
    "\n",
    "    \n",
    "# Use a regression-based approach to detect anaomalies\n",
    "# e.g. linear / polynomial regression\n",
    "# see: https://scikit-learn.org/stable/modules/linear_model.html\n",
    "def regression_based_scores(X, test_start):\n",
    "    score = np.zeros(len(X))\n",
    "    # TODO: fill in code here as needed \n",
    "    from sklearn.ensemble import GradientBoostingRegressor\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from scipy.ndimage import uniform_filter1d\n",
    "    \n",
    "    # Preprocessing: Z-score normalize\n",
    "    scaler = StandardScaler()\n",
    "    X_normalized = scaler.fit_transform(X[:test_start])\n",
    "    X_all = scaler.transform(X)\n",
    "    \n",
    "    # Use multiple lagged values as features\n",
    "    n_lags = 3\n",
    "    X_train = X_all[:test_start-n_lags]\n",
    "    y_train = X_all[n_lags:test_start, -1]\n",
    "    \n",
    "    # Use Gradient Boosting for better predictions\n",
    "    model = GradientBoostingRegressor(\n",
    "        n_estimators=100,\n",
    "        max_depth=3,\n",
    "        learning_rate=0.1,\n",
    "        random_state=42\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict for all data\n",
    "    y_pred = model.predict(X_all[:-n_lags])\n",
    "    y_actual = X_all[n_lags:, -1]\n",
    "    \n",
    "    # Use absolute residuals (more robust)\n",
    "    residuals = np.abs(y_actual - y_pred)\n",
    "    \n",
    "    # Apply smoothing to reduce noise\n",
    "    window_smooth = max(5, X.shape[1] // 4)\n",
    "    residuals_smooth = uniform_filter1d(residuals, size=window_smooth)\n",
    "    \n",
    "    score[n_lags:len(residuals_smooth)+n_lags] = residuals_smooth\n",
    "    \n",
    "    # Pay attention to using argmin or argmax!\n",
    "    predicted = test_start + np.argmax(score[test_start:])\n",
    "    return score, predicted\n",
    "\n",
    "    \n",
    "# Use a forecasting-based approach to detect anaomalies\n",
    "# e.g. ARIMA, Prophet\n",
    "# see: https://medium.com/@aeon.toolkit/blazingly-fast-arima-with-aeon-9796d7015cdd\n",
    "def forecasting_based_scores(X, test_start):\n",
    "    score = np.zeros(len(X))\n",
    "    # TODO: fill in code here as needed \n",
    "\n",
    "    from sklearn.neighbors import LocalOutlierFactor, NearestNeighbors\n",
    "    from sklearn.linear_model import Ridge\n",
    "    from sklearn.preprocessing import RobustScaler\n",
    "    from scipy.ndimage import gaussian_filter1d\n",
    "    import warnings\n",
    "    warnings.filterwarnings('ignore')\n",
    "    \n",
    "    # === 1. Feature Engineering with temporal features ===\n",
    "    features = []\n",
    "    for i in range(len(X)):\n",
    "        window = X[i]\n",
    "        feat = [\n",
    "            np.mean(window),\n",
    "            np.std(window),\n",
    "            window[-1],\n",
    "            window[-1] - window[0],\n",
    "            np.max(window) - np.min(window),\n",
    "            np.percentile(window, 75) - np.percentile(window, 25),\n",
    "        ]\n",
    "        diffs = np.diff(window)\n",
    "        feat.extend([np.mean(diffs), np.std(diffs), np.max(np.abs(diffs))])\n",
    "        \n",
    "        # Add second derivative\n",
    "        diffs2 = np.diff(diffs)\n",
    "        if len(diffs2) > 0:\n",
    "            feat.extend([np.mean(diffs2), np.std(diffs2)])\n",
    "        else:\n",
    "            feat.extend([0, 0])\n",
    "        \n",
    "        features.append(feat)\n",
    "    \n",
    "    X_features = np.array(features)\n",
    "    \n",
    "    # === 2. Scaling ===\n",
    "    scaler = RobustScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X[:test_start - window_size + 1])\n",
    "    X_all_scaled = scaler.transform(X_features)\n",
    "    \n",
    "    all_scores = []\n",
    "    \n",
    "    # === 3. Prediction residuals ensemble ===\n",
    "    for lag in [1, 2, 3, 5]:\n",
    "        if test_start > lag + 10:\n",
    "            X_in = X_train_scaled[:-lag]\n",
    "            y_out = X_train_scaled[lag:]\n",
    "            \n",
    "            model = Ridge(alpha=1.0)\n",
    "            model.fit(X_in, y_out)\n",
    "            \n",
    "            pred_scores = np.zeros(len(X_all_scaled))\n",
    "            for i in range(len(X_all_scaled) - lag):\n",
    "                pred = model.predict(X_all_scaled[i:i+1])\n",
    "                actual = X_all_scaled[i + lag]\n",
    "                pred_scores[i + lag] = np.linalg.norm(actual - pred)\n",
    "            \n",
    "            if np.max(pred_scores) > 0:\n",
    "                pred_scores = pred_scores / np.max(pred_scores)\n",
    "            all_scores.append(pred_scores)\n",
    "    \n",
    "    # === 4. LOF on temporal context (like cluster method) ===\n",
    "    for n_neighbors in [10, 20, 30]:\n",
    "        try:\n",
    "            lof = LocalOutlierFactor(n_neighbors=n_neighbors, novelty=True, contamination=0.01)\n",
    "            lof.fit(X_train_scaled)\n",
    "            lof_scores = -lof.decision_function(X_all_scaled)\n",
    "            lof_scores = (lof_scores - np.min(lof_scores)) / (np.max(lof_scores) - np.min(lof_scores) + 1e-10)\n",
    "            all_scores.append(lof_scores)\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    # === 5. kNN distance (key component from cluster method) ===\n",
    "    for k in [5, 10, 15]:\n",
    "        knn = NearestNeighbors(n_neighbors=k)\n",
    "        knn.fit(X_train_scaled)\n",
    "        distances, _ = knn.kneighbors(X_all_scaled)\n",
    "        avg_dist = np.mean(distances, axis=1)\n",
    "        avg_dist = (avg_dist - np.min(avg_dist)) / (np.max(avg_dist) - np.min(avg_dist) + 1e-10)\n",
    "        all_scores.append(avg_dist)\n",
    "    \n",
    "    # === 6. Combine ===\n",
    "    combined = np.mean(all_scores, axis=0)\n",
    "    \n",
    "    # === 7. Heavy smoothing (same as cluster) ===\n",
    "    window_size = max(20, X.shape[1])\n",
    "    score = gaussian_filter1d(combined, sigma=window_size/3)\n",
    "    \n",
    "    # Zero training\n",
    "    score[:test_start] = 0\n",
    "    \n",
    "    # Pay attention to using argmin or argmax!\n",
    "    predicted = test_start + np.argmax(score[test_start:])\n",
    "    return score, predicted\n",
    "\n",
    "\n",
    "# Use a clustering approach to detect anaomalies\n",
    "# e.g., DBSCAN\n",
    "# see: https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html\n",
    "def cluster_based_scores(X, test_start):\n",
    "    \n",
    "    import numpy as np\n",
    "    from sklearn.cluster import KMeans\n",
    "    from sklearn.neighbors import LocalOutlierFactor, NearestNeighbors\n",
    "    from sklearn.preprocessing import RobustScaler, MinMaxScaler\n",
    "    from scipy.ndimage import gaussian_filter1d\n",
    "    import warnings\n",
    "    warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "\n",
    "    # === 1. Vectorized Feature Engineering ===\n",
    "\n",
    "    f_std = np.std(X, axis=1)\n",
    "    f_trend = X[:, -1] - X[:, 0]\n",
    "    f_range = np.ptp(X, axis=1) \n",
    "    f_iqr = np.percentile(X, 75, axis=1) - np.percentile(X, 25, axis=1)\n",
    "    \n",
    "    diffs = np.diff(X, axis=1)\n",
    "    f_d_mean = np.mean(diffs, axis=1)\n",
    "    f_d_std = np.std(diffs, axis=1)\n",
    "    f_d_max = np.max(np.abs(diffs), axis=1)\n",
    "    \n",
    "    # Peak Location\n",
    "    f_argmax = np.argmax(X, axis=1).astype(float) \n",
    "    \n",
    "    # Assymetrie\n",
    "    mid_point = X.shape[1] // 2\n",
    "    left_std = np.std(X[:, :mid_point], axis=1)\n",
    "    right_std = np.std(X[:, mid_point:], axis=1)\n",
    "    f_asymmetry = left_std - right_std \n",
    "\n",
    "    # Center of Mass\n",
    "    indices = np.arange(X.shape[1])\n",
    "    \n",
    "    X_pos = X - np.min(X, axis=1, keepdims=True) + 1e-5\n",
    "    f_center_mass = np.sum(indices * X_pos, axis=1) / np.sum(X_pos, axis=1)\n",
    "\n",
    "    X_features = np.column_stack([\n",
    "        f_std, f_trend, f_range, f_iqr,\n",
    "        f_d_mean, f_d_std, f_d_max,\n",
    "        f_argmax,      \n",
    "        f_asymmetry,   \n",
    "        f_center_mass  \n",
    "    ])\n",
    "\n",
    "    # === 2. Strict Train/Test Split (No Leakage) ===\n",
    "\n",
    "    window_size = X.shape[1]\n",
    "    train_end = max(0, test_start - window_size + 1)\n",
    "\n",
    "    # RobustScaler \n",
    "\n",
    "    scaler = RobustScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_features[:train_end])\n",
    "    X_all_scaled = scaler.transform(X_features)\n",
    "\n",
    "    # === 3. Ensemble ===\n",
    "\n",
    "    all_scores = []\n",
    "\n",
    "    # Method A: LOF\n",
    "    \n",
    "    for n in [10, 30]: \n",
    "        lof = LocalOutlierFactor(n_neighbors=n, novelty=True, contamination=0.001)\n",
    "        lof.fit(X_train_scaled)\n",
    "        all_scores.append(-lof.decision_function(X_all_scaled))\n",
    "\n",
    "    # Method B: KMeans Distance\n",
    "\n",
    "    for n in [5, 10]: \n",
    "        kmeans = KMeans(n_clusters=n, random_state=42, n_init=10)\n",
    "        kmeans.fit(X_train_scaled)\n",
    "        distances = kmeans.transform(X_all_scaled)\n",
    "        all_scores.append(np.min(distances, axis=1))\n",
    "\n",
    "    # Method C: kNN Distance\n",
    "\n",
    "    for k in [5, 15]:\n",
    "        knn = NearestNeighbors(n_neighbors=k)\n",
    "        knn.fit(X_train_scaled)\n",
    "        dists, _ = knn.kneighbors(X_all_scaled)\n",
    "        all_scores.append(np.mean(dists, axis=1))\n",
    "\n",
    "    # === 4. Robust Combination ===\n",
    "\n",
    "    normalized_scores = []\n",
    "    score_scaler = MinMaxScaler() # Можно заменить на RobustScaler если много шума\n",
    "\n",
    "    for s in all_scores:\n",
    "\n",
    "        cap = np.percentile(s, 99) \n",
    "        s_clipped = np.clip(s, a_min=None, a_max=cap)\n",
    "        s_norm = score_scaler.fit_transform(s_clipped.reshape(-1, 1)).flatten()\n",
    "        normalized_scores.append(s_norm)\n",
    "\n",
    "    combined = np.mean(normalized_scores, axis=0) # Можно попробовать np.max\n",
    "\n",
    "    # === 5. Smoothing ===\n",
    "\n",
    "    filter_sigma = max(5, window_size / 3) \n",
    "    score = gaussian_filter1d(combined, sigma=filter_sigma)\n",
    "\n",
    "    # Zero out training region\n",
    "\n",
    "    score[:test_start] = 0\n",
    "    \n",
    "    predicted = test_start + np.argmax(score[test_start:])\n",
    "    return score, predicted\n",
    "\n",
    "\n",
    "# Use a statistics approach to detect anaomalies\n",
    "# e.g., KDE, IQR\n",
    "def statistics_based_scores(data, X, test_start):   \n",
    "    score = np.zeros(len(X))\n",
    "    # TODO: fill in code here as needed \n",
    "\n",
    "    from scipy.stats import gaussian_kde, zscore\n",
    "    from scipy.ndimage import gaussian_filter1d\n",
    "    from sklearn.preprocessing import RobustScaler\n",
    "    import warnings\n",
    "    warnings.filterwarnings('ignore')\n",
    "    \n",
    "    # === 1. Feature Engineering ===\n",
    "    features = []\n",
    "    for i in range(len(X)):\n",
    "        window = X[i]\n",
    "        feat = [\n",
    "            np.mean(window),\n",
    "            np.std(window),\n",
    "            window[-1],\n",
    "            window[-1] - window[0],\n",
    "            np.max(window) - np.min(window),\n",
    "            np.percentile(window, 75) - np.percentile(window, 25),\n",
    "        ]\n",
    "        diffs = np.diff(window)\n",
    "        feat.extend([np.mean(diffs), np.std(diffs), np.max(np.abs(diffs))])\n",
    "        features.append(feat)\n",
    "    \n",
    "    X_features = np.array(features)\n",
    "    \n",
    "    # === 2. Scale based on training data ===\n",
    "    scaler = RobustScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X[:test_start - window_size + 1])\n",
    "    X_all_scaled = scaler.transform(X_features)\n",
    "    \n",
    "    all_scores = []\n",
    "    \n",
    "    # === 3. Multi-dimensional KDE ===\n",
    "    try:\n",
    "        kde = gaussian_kde(X_train_scaled.T)\n",
    "        likelihoods = kde(X_all_scaled.T)\n",
    "        kde_score = 1 / (likelihoods + 1e-10)\n",
    "        kde_score = (kde_score - np.min(kde_score)) / (np.max(kde_score) - np.min(kde_score) + 1e-10)\n",
    "        all_scores.append(kde_score)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # === 4. Mahalanobis-like distance ===\n",
    "    train_mean = np.mean(X_train_scaled, axis=0)\n",
    "    train_cov = np.cov(X_train_scaled.T)\n",
    "    try:\n",
    "        inv_cov = np.linalg.inv(train_cov + np.eye(train_cov.shape[0]) * 1e-6)\n",
    "        mahal_scores = []\n",
    "        for i in range(len(X_all_scaled)):\n",
    "            diff = X_all_scaled[i] - train_mean\n",
    "            mahal = np.sqrt(diff @ inv_cov @ diff.T)\n",
    "            mahal_scores.append(mahal)\n",
    "        mahal_scores = np.array(mahal_scores)\n",
    "        mahal_scores = (mahal_scores - np.min(mahal_scores)) / (np.max(mahal_scores) - np.min(mahal_scores) + 1e-10)\n",
    "        all_scores.append(mahal_scores)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # === 5. Per-feature Z-score outlier detection ===\n",
    "    z_combined = np.zeros(len(X_all_scaled))\n",
    "    for col in range(X_all_scaled.shape[1]):\n",
    "        train_mean_col = np.mean(X_train_scaled[:, col])\n",
    "        train_std_col = np.std(X_train_scaled[:, col]) + 1e-10\n",
    "        z_scores = np.abs((X_all_scaled[:, col] - train_mean_col) / train_std_col)\n",
    "        z_combined += z_scores\n",
    "    z_combined = (z_combined - np.min(z_combined)) / (np.max(z_combined) - np.min(z_combined) + 1e-10)\n",
    "    all_scores.append(z_combined)\n",
    "    \n",
    "    # === 6. IQR-based multi-feature ===\n",
    "    iqr_combined = np.zeros(len(X_all_scaled))\n",
    "    for col in range(X_train_scaled.shape[1]):\n",
    "        Q1 = np.percentile(X_train_scaled[:, col], 25)\n",
    "        Q3 = np.percentile(X_train_scaled[:, col], 75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower = Q1 - 1.5 * IQR\n",
    "        upper = Q3 + 1.5 * IQR\n",
    "        for i in range(len(X_all_scaled)):\n",
    "            if X_all_scaled[i, col] < lower:\n",
    "                iqr_combined[i] += lower - X_all_scaled[i, col]\n",
    "            elif X_all_scaled[i, col] > upper:\n",
    "                iqr_combined[i] += X_all_scaled[i, col] - upper\n",
    "    if np.max(iqr_combined) > 0:\n",
    "        iqr_combined = iqr_combined / np.max(iqr_combined)\n",
    "    all_scores.append(iqr_combined)\n",
    "    \n",
    "    # === 7. Combine all scores ===\n",
    "    combined = np.mean(all_scores, axis=0)\n",
    "    \n",
    "    # === 8. Heavy smoothing ===\n",
    "    window_size = max(20, X.shape[1])\n",
    "    score = gaussian_filter1d(combined, sigma=window_size/3)\n",
    "    \n",
    "    # Zero training region\n",
    "    score[:test_start] = 0\n",
    "    \n",
    "    # Pay attention to using argmin or argmax!\n",
    "    predicted = test_start + np.argmax(score[test_start:])\n",
    "    return score, predicted\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "embedded-graphics",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_score = 0\n",
    "predictions = []\n",
    "ids = []\n",
    "i = 1\n",
    "\n",
    "for file in file_list:\n",
    "    if \"Anomaly\" in str(file):\n",
    "        file_name = file.split('.')[0]\n",
    "        name, test_start, data, anomaly = read_series(file, locations)\n",
    "\n",
    "        period = find_dominant_window_sizes(data[:test_start])\n",
    "        print(\"Dominant Period\", period)\n",
    "        \n",
    "        window_size = np.int32(period)\n",
    "        X = sliding_window(data, window_size)\n",
    "\n",
    "        #score, predicted = nearest_neighbor_based_score(X, test_start)            \n",
    "\n",
    "        # TODO: uncomment as needed\n",
    "        #score, predicted = nearest_neighbor_based_score(X, test_start) #SCORES 4\n",
    "        #score, predicted = classifcation_based_scores(X, test_start) #\n",
    "        #score, predicted = regression_based_scores(X, test_start) #SCORES 10      \n",
    "        #score, predicted = forecasting_based_scores(X, test_start) #SCORES 6 (5.5 on Kaggle)\n",
    "        score, predicted = cluster_based_scores(X, test_start) #OUR BEST WARRIOR\n",
    "        #score, predicted = statistics_based_scores(data, X, test_start) #SCORES 9.7\n",
    "        \n",
    "        predictions.append(predicted)\n",
    "        ids.append(file_name)        \n",
    "        score[:test_start] = np.nan\n",
    "\n",
    "        # Visualize the predicted anomaly\n",
    "        visualize_with_anomaly_score(\n",
    "             data, score, test_start, predicted, anomaly, name)\n",
    "        \n",
    "        # Only check those, where the anomaly was annotated\n",
    "        if (anomaly[0] > -1):\n",
    "            anomaly = (anomaly[0] + anomaly[1]) // 2\n",
    "            total_score += abs(anomaly - predicted) / (anomaly)\n",
    "            i = i + 1\n",
    "            \n",
    "        print(\"Current Score: \", (total_score / i) * 100)\n",
    "\n",
    "        \n",
    "print(\"\\tTotal score:\", (total_score / len(locations)) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c22512c",
   "metadata": {},
   "source": [
    "# Submit your solution to Kaggle\n",
    "\n",
    "You must make at least **3 different submissions to Kaggle**.\n",
    "\n",
    "<div class=\"alert alert-success alertsuccess\" style=\"margin-top: 20px\">\n",
    "Create a submission named `submission.csv` using your model and upload it to kaggle:\n",
    "\n",
    "- https://www.kaggle.com/competitions/time-series-anomaly-detection-exercise-WS-25-26\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3f23b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a submission\n",
    "submission = pd.DataFrame({'ID': ids,'PREDICTED': predictions})\n",
    "\n",
    "# Visualize the first 5 rows\n",
    "display(submission)\n",
    "\n",
    "filename = 'baseline_submission_phase_1.csv'\n",
    "submission.to_csv(filename,index=False)\n",
    "print('Saved file: ' + filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e02453-9719-4a01-805d-5b092116f406",
   "metadata": {},
   "source": [
    "<hr> \n",
    "\n",
    "# Submit via Moodle:\n",
    "- Your notebook with three approaches (or three notebooks)\n",
    "- A html export of this notebook (or the three notebooks)\n",
    "- The three submissions, which you used for kaggle\n",
    "\n",
    "### You must hand in this exercise via moodle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5350586d-9a64-4758-b3e1-e818af24cc6e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
